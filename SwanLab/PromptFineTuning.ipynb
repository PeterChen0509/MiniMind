{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b6f1a0",
   "metadata": {},
   "source": [
    "# 查看有监督微调的数据集格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c745c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['conversations'])\n",
      "{'conversations': [{'role': 'user', 'content': '请告诉我在中国古代的“四大发明”是什么？'}, {'role': 'assistant', 'content': '中国古代的“四大发明”是指造纸术、印刷术、火药和指南针。这四项发明对世界文明的发展产生了深远的影响：\\n\\n1. **造纸术**：据史书记载，东汉时期的蔡伦改进了造纸工艺，使得纸张的生产更加便捷、成本更低，质量也更加优良。这一发明极大地促进了文化的传播和保存。\\n\\n2. **印刷术**：中国古代的印刷术最早可以追溯到唐代的雕版印刷，到了宋代发展出了活字印刷技术。印刷术的发明极大地促进了知识的传播，降低了书籍的成本，对教育和文化的发展起到了重要的推动作用。\\n\\n3. **火药**：火药最初是在唐代被发现的，最初可能用于医疗或炼金术。到了宋代，人们开始将火药用于军事目的，发明了各种火器。火药的发明改变了战争的面貌，对世界军事技术的发展产生了重大影响。\\n\\n4. **指南针**：指南针最初在中国被用于风水测量，后来逐渐发展成为航海导航的重要工具。这一发明极大地促进了海上航行技术的发展，对于新航路的开辟和世界地理大发现起到了关键作用。\\n\\n这四项发明不仅在中国历史上占有重要地位，而且对全世界的科技进步和文明发展都产生了深远的影响。'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "pretrain_dataset_path = r\"D:\\MiniMind\\dataset\\sft_mini_512.jsonl\"\n",
    "with open(pretrain_dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_num, line in enumerate(f,1):\n",
    "        data = json.loads(line.strip())\n",
    "        break\n",
    "\n",
    "print(data.keys())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0cdf7",
   "metadata": {},
   "source": [
    "# 有监督微调数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d351bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\minimind\\lib\\site-packages\\torch\\cuda\\__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_length=1024):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = self.load_data(jsonl_path)\n",
    "        # tokenizer把文本转换成token id\n",
    "        # input_ids表示分词后得到的token id序列\n",
    "        # add_special_tokens=False表示不添加特殊字符\n",
    "        self.bos_id = tokenizer('<im_start>assistant', add_special_tokens=False).input_ids\n",
    "        self.eos_id = tokenizer('<im_end>', add_special_tokens=False).input_ids\n",
    "    def __len__(self):\n",
    "        return len(self.samples) # 返回样本数量\n",
    "    def load_data(self, path):\n",
    "        # 从jsonl文件加载对话数据\n",
    "        samples = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            # 行号默认从1开始\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                data = json.loads(line.strip()) # 每行为一个JSON对象\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "    def _create_chat_prompt(self, conversations):\n",
    "        # 对话轮构成符合ChatML格式的字符串，每一轮用户、助手对话被标注为'user'和'assistant'\n",
    "        messages = []\n",
    "        for i, turn in enumerate(conversations):\n",
    "            role = 'user' if i % 2 == 0 else 'assistant' # 偶数轮为用户，奇数轮为助手\n",
    "            messages.append({\"role\": role, \"content\": turn[\"content\"]})\n",
    "        # 返回字符串形式的prompt,而非直接tokenize\n",
    "        return self.tokenizer.apply_chat_template(\n",
    "            messages, # 结构化对话列表\n",
    "            tokenize=False, # 是否分词\n",
    "            add_generation_prompt=False # 是否在末尾自动添加一个\"assistant开始说话\"的提示\n",
    "        )\n",
    "    def _generate_loss_mask(self, input_ids):\n",
    "        # 构建损失掩码，只有assistant的回答部分才参与loss计算\n",
    "        # 找出每一段assistant的响应，在其<im_start>assistant和<im_end>之间设置loss_mask为1\n",
    "        loss_mask = [0] * len(input_ids)\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            # 找assistant开头标志\n",
    "            if input_ids[i: i+len(self.bos_id)] == self.bos_id:\n",
    "                start = i + len(self.bos_id) # 答案起点\n",
    "                end = start\n",
    "                while end < len(input_ids):\n",
    "                    # 查找assistant的回答终止符<im_end>\n",
    "                    if input_ids[end: end+len(self.eos_id)] == self.eos_id:\n",
    "                        break\n",
    "                    end += 1\n",
    "                # 为assistant回答部分(从start+1到end之间)设置loss_mask\n",
    "                for j in range(start+1, min(end+len(self.eos_id)+1, self.max_length)):\n",
    "                    loss_mask[j] = 1\n",
    "                # 跳过到下一个segment\n",
    "                i = end + len(self.eos_id) if end < len(input_ids) else len(input_ids)\n",
    "            else:\n",
    "                i += 1\n",
    "        return loss_mask\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        # 构建ChatML格式prompt\n",
    "        prompt = self._create_chat_prompt(sample['conversations'])\n",
    "        # 分词并截断，确保长度<=max_length\n",
    "        input_ids = self.tokenizer(prompt).input_ids[:self.max_length]\n",
    "        # 右侧填充pad_token直到max_length长度\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n",
    "        # 生成动态loss_mask, 仅对assistant响应位置计算loss\n",
    "        loss_mask = self._generate_loss_mask(input_ids)\n",
    "\n",
    "        # 构建训练样本\n",
    "        # 模型输入为前n-1个token，预测目标为第2到第n个token\n",
    "        X = torch.tensor(input_ids[:-1], dtype=torch.long) # 输入序列\n",
    "        Y = torch.tensor(input_ids[1:], dtype=torch.long) # 目标标签（shifted）\n",
    "        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long) # 对齐 Y 的位置（从第一个预测 token 开始）\n",
    "\n",
    "        return X, Y, loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c69a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\minimind\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 构建数据集加载器\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 512\n",
    "data_path = r\"D:\\MiniMind\\dataset\\sft_mini_512.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"D:\\MiniMind\\model\")\n",
    "train_ds = SFTDataset(data_path, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=2, # 一个batch有2个样本\n",
    "    pin_memory=True, # 如果使用GPU，则将数据加载到显存中\n",
    "    drop_last=False, # 如果最后一批不满batch_size，不丢掉\n",
    "    shuffle=False, # 不打乱数据顺序\n",
    "    num_workers=0, # 几个子进程来加载数据，0就是主进程加载,>0就是哆嗦进程并行加载\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596a74f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607362\n",
      "[torch.Size([2, 511]), torch.Size([2, 511]), torch.Size([2, 511])]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "for item in train_loader:\n",
    "    print([i.shape for i in item])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e8c53",
   "metadata": {},
   "source": [
    "# 有监督微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e91ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (X, Y, loss_mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> 5\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      6\u001b[0m     Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "    X = X.to(args.device)\n",
    "    Y = Y.to(args.device)\n",
    "    loss_mask = loss_mask.to(args.device)\n",
    "    lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch, args.learning_rate)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    with ctx:\n",
    "        res = model(X)\n",
    "        loss = loss_fct(\n",
    "            res.logits.view(-1, res.logits.size(-1)),\n",
    "            Y.view(-1)\n",
    "        ).view(Y.size())\n",
    "\n",
    "        loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "        loss += res.aux_loss\n",
    "        loss = loss / args.accumulation_steps\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    if (step + 1) % args.accumulation_steps == 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        optimizer.zero_grad(set_to_one=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83b58d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
