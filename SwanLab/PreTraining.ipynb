{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d88757b",
   "metadata": {},
   "source": [
    "# 预训练数据集格式 \n",
    "\n",
    "预训练用大规模的无监督语料对模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daede1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text'])\n",
      "{'text': '<|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\\n文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|>'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "pretrain_dataset_path = r'D:\\MiniMind\\dataset\\pretrain_hq.jsonl'\n",
    "with open(pretrain_dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        data = json.loads(line.strip())\n",
    "        break\n",
    "\n",
    "print(data.keys())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9abe4",
   "metadata": {},
   "source": [
    "# 准备预训练数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "587dd9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=512):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer # 分词器，把文本转为token ID\n",
    "        self.max_length = max_length # 每条样本的最大token长度\n",
    "        self.samples = self.load_data(data_path) # 加载数据\n",
    "    def load_data(self, path):\n",
    "        # 从文件中加载数据，每一行为一条JSON格式的样本\n",
    "        samples = []\n",
    "        with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                # 读取每一行，解析成字典结构\n",
    "                data = json.loads(line.strip())\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "    def __len__(self):\n",
    "        # 返回样本数量\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        # 将样本中的文本字段进行tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            str(sample['text']),\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # 获取input_ids张量，去除batch维度\n",
    "        input_ids = encoding.input_ids.squeeze()\n",
    "\n",
    "        # 计算loss_mask: pad位置不参与loss\n",
    "        loss_mask = (input_ids != self.tokenizer.pad_token_id)\n",
    "\n",
    "        # 用前一个token预测下一个\n",
    "        X = torch.tensor(input_ids[:-1], dtype=torch.long) # 输入：[0, ..., n-2]\n",
    "        Y = torch.tensor(input_ids[1:], dtype=torch.long) # 目标：[1, ..., n-1]\n",
    "        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)\n",
    "\n",
    "        return X, Y, loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffa8f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 512\n",
    "data_path = r\"D:\\MiniMind\\dataset\\pretrain_hq.jsonl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"D:\\MiniMind\\model\")\n",
    "train_ds = PretrainDataset(data_path, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c38c3e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706552\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e7e6e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([2, 511]), torch.Size([2, 511]), torch.Size([2, 511])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22028\\2296140921.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(input_ids[:-1], dtype=torch.long) # 输入：[0, ..., n-2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22028\\2296140921.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(input_ids[1:], dtype=torch.long) # 目标：[1, ..., n-1]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22028\\2296140921.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "for item in train_loader:\n",
    "    print([i.shape for i in item])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e9084c",
   "metadata": {},
   "source": [
    "# 开始预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f86f0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 定义交叉熵损失函数\n",
    "import torch.nn as nn\n",
    "from contextlib import nullcontext\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# 在不改变代码结构的前提下，CPU上什么都不做，GPU上就自动混合精度\n",
    "# CPU不支持float16加速运算，GPU上可以\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.cuda.amp.autocast()\n",
    "\n",
    "# 遍历训练数据加载器\n",
    "for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "    X = X.to(args.device)\n",
    "    Y = Y.to(args.device)\n",
    "    loss_mask = loss_mask.to(args.device) # 形状都是(batch_size, seq_len)\n",
    "\n",
    "    # 自定义学习率调度函数更新学习率\n",
    "    lr = get_lr(\n",
    "        epoch * iter_per_epoch + step, # 当前训练步数\n",
    "        args.epochs * iter_per_epoch, # 总训练步数\n",
    "        args.learning_rate # 初始学习率\n",
    "    )\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # 自动混合精度上下文\n",
    "    with ctx: \n",
    "        res = model(X)\n",
    "\n",
    "        # 计算token级别的交叉熵损失\n",
    "        loss = loss_fct(\n",
    "            res.logits.view(-1, res.logits.size(-1)), # logits是softmax之前未归一化的分数\n",
    "            Y.view(-1)\n",
    "        ).view(Y.size())\n",
    "\n",
    "        # 仅在非pad的位置计算损失\n",
    "        loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "\n",
    "        # 加入模型可能返回的辅助损失\n",
    "        loss += res.aux_loss\n",
    "\n",
    "        # 梯度累计\n",
    "        loss = loss / args.accumulation_steps\n",
    "\n",
    "    # PyTorch自带一个自动混合精度模块，在float16混合精度下训练更稳定\n",
    "    # GPU上直接用autocast 混合精度fp16有时会下溢，有时也上溢inf\n",
    "    from torch.amp import GradScaler\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # 给loss乘上一个缩放因子，放大再反向传播\n",
    "    scaler.scale(loss).backward() \n",
    "\n",
    "    # 累计到一定步数后进行一次参数更新\n",
    "    if (step + 1) % args.accumulation_steps == 0:\n",
    "        # 把刚刚放大的梯度恢复到原本的尺度\n",
    "        scaler.unscale_(optimizer)\n",
    "        # 梯度剪裁必须基于真实梯度, 末尾_表示原地操作\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip) # （要剪的参数集合， 阈值）\n",
    "\n",
    "        # 参数更新\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # 更新scaler的缩放因子S\n",
    "        scaler.update()\n",
    "\n",
    "        # 清空梯度，把.grad设置为None(True). False的话是把梯度张量设置为0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e26f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da48e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10b101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
